# A Comparative Study of 10 LLMs
This project analyzes and visualizes 10 prominent large language models using open benchmark data — including Chatbot Arena Elo scores, Stanford HELM, GPQA, SWE-Bench, and AIME — alongside input/output token costs. The interactive dashboard enables users to compare model performance per dollar, filter by knowledge cutoff, and explore detailed per-model metrics.
